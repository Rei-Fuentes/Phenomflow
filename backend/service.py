"""
PhenomFlow v3.0 - Service Layer MEJORADO + Flask API
=====================================================

MEJORAS EN ESTA VERSI√ìN:
1. Endpoint Flask /analyze/enhanced con contexto de investigaci√≥n
2. Generaci√≥n autom√°tica de body maps por estructura
3. CORS habilitado para frontend
4. Soporte dual: Claude Sonnet 4.5 (recomendado) + OpenAI (fallback)
5. Prompts v3.0 completos embebidos

AUTOR: PhenomFlow v3.0 Team
FECHA: 2024-12-08
"""

from typing import List, Dict, Any, Optional
import os
import json
import time
import random
from datetime import datetime
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS

# Load environment variables from project root
basedir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
load_dotenv(os.path.join(basedir, ".env"))

# =============================================================================
# FLASK APP SETUP
# =============================================================================

app = Flask(__name__)
CORS(app)  # Habilitar CORS para permitir requests desde localhost:3000

# =============================================================================
# CONFIGURACI√ìN DE CLIENTE (Claude preferido, OpenAI como fallback)
# =============================================================================

USE_CLAUDE = os.getenv("USE_CLAUDE", "true").lower() == "true"

if USE_CLAUDE:
    try:
        import anthropic
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        MODEL = "claude-sonnet-4-20250514"
        print("‚úì Usando Claude Sonnet 4.5 (recomendado para v3.0)")
    except Exception as e:
        print(f"‚ö† Claude no disponible ({e}), usando OpenAI como fallback")
        USE_CLAUDE = False

if not USE_CLAUDE:
    from openai import OpenAI
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    MODEL = "gpt-4o"
    print("‚ö† Usando GPT-4o (menor precisi√≥n que Claude para v3.0)")


# =============================================================================
# CARGA DE PROMPTS COMPLETOS v3.0
# =============================================================================

def load_prompt_parts():
    """
    Carga los 3 archivos de prompts v3.0 desde /prompts/ o define inline.
    """
    prompts_dir = os.path.join(basedir, "prompts")
    
    try:
        with open(f"{prompts_dir}/PHENOMFLOW_v3_PARTE_1_ANALISIS_INDIVIDUAL.txt", "r", encoding="utf-8") as f:
            PARTE_1 = f.read()
        with open(f"{prompts_dir}/PHENOMFLOW_v3_PARTE_2_SINTESIS_CROSSCASE.txt", "r", encoding="utf-8") as f:
            PARTE_2 = f.read()
        with open(f"{prompts_dir}/PHENOMFLOW_v3_PARTE_3_FINAL_VALIDACION.txt", "r", encoding="utf-8") as f:
            PARTE_3 = f.read()
        print("‚úì Prompts v3.0 cargados desde archivos")
        return PARTE_1, PARTE_2, PARTE_3
    except FileNotFoundError:
        print("‚ö† Archivos de prompts no encontrados, usando versi√≥n embebida (simplificada)")
        return get_embedded_prompts()


def get_embedded_prompts():
    """
    Versi√≥n embebida de prompts v3.0 (simplificada por l√≠mites de tama√±o).
    Para versi√≥n COMPLETA, usar archivos .txt generados anteriormente.
    """
    
    PARTE_1_EMBEDDED = """
================================================================================
PHENOMFLOW v3.0 - AN√ÅLISIS INDIVIDUAL FENOMENOL√ìGICO RIGUROSO
PARTE 1: PREPARACI√ìN Y AN√ÅLISIS DIMENSIONAL (6 Dimensiones)
================================================================================

PRINCIPIOS FUNDAMENTALES:

1. **EPOCH√â RIGUROSA**: NO interpretes causalmente. Reporta solo lo VIVIDO.
   - ‚ùå PROHIBIDO: "activaci√≥n amigdalar", "cortisol", "sistema nervioso simp√°tico"
   - ‚úì PERMITIDO: "sent√≠ escalofr√≠os", "mi coraz√≥n lat√≠a fuerte"

2. **EMERGENCIA**: C√≥digos emergen de datos (bottom-up), NO categor√≠as a priori.

3. **VARIABILIDAD**: Respeta diferencias individuales, no fuerces homogeneidad.

4. **TRIANGULACI√ìN**: Cada c√≥digo ‚â•2 participantes (si N‚â•3).

5. **GRANULARIDAD MULTINIVEL**: 4 niveles jer√°rquicos en codebook.

---

## SECCI√ìN 0: PREPARACI√ìN DEL VERBATIM

### 0.1 Detecci√≥n de Declaraciones No-Descriptivas

Identifica y MARCA (no elimines a√∫n) declaraciones "sat√©lite":
- Generalizaciones: "siempre me pasa", "la gente suele"
- Evaluaciones: "fue horrible", "estuvo bien"
- Teor√≠as: "creo que fue adrenalina", "es por el estr√©s"
- Causales: "porque ten√≠a miedo", "dado que..."

**Acci√≥n**: Marca con ‚ö†Ô∏è y etiqueta tipo (GENERALIZACI√ìN/EVALUACI√ìN/TEOR√çA/CAUSAL)

### 0.2 Evaluaci√≥n de Confiabilidad (‚úì‚úì‚úì Sistema)

Para CADA segmento del verbatim, eval√∫a con estos 7 criterios:

1. **Detalles sensoriales espec√≠ficos** (color, textura, sonido, sabor, olor, temperatura)
2. **Coherencia temporal** (secuencia l√≥gica de eventos)
3. **Respuesta no-inductiva** (no repite palabras del entrevistador)
4. **Met√°foras inventadas** (no clich√©s: "como una monta√±a rusa" ‚ùå, "como si mi pecho se abriera hacia todos lados" ‚úì)
5. **Pausas/titubeos** expl√≠citos ("...pausa...", "uhm", "es dif√≠cil de describir")
6. **Oraciones cortas/fragmentadas** (se√±al de re-acceso genuino vs. narrativa construida)
7. **Verbos de acci√≥n** (vs. verbos de estado: "sent√≠ que X se mov√≠a" ‚úì vs "era X" ‚ùå)

**Sistema de marcado**:
- ‚úì‚úì‚úì ALTA confiabilidad: ‚â•4 criterios cumplidos
- ‚úì‚úì MEDIA confiabilidad: 2-3 criterios
- ‚úì BAJA confiabilidad: 0-1 criterios

### 0.3 Reorganizaci√≥n Cronol√≥gica

**MALLA GRUESA** (visi√≥n general):
Identifica fases temporales generales

**MALLA FINA** (momento a momento):
Reconstruye secuencia detallada con marcadores temporales

### 0.4 Segmentaci√≥n en Unidades de Significado

**Criterio**: Cambio en foco atencional, dimensi√≥n fenomenol√≥gica o contenido.

**Formato obligatorio**: `[U#-P##]` (ej: [U1-P21] = Unidad 1 del Participante 21)

---

## SECCI√ìN 1: AN√ÅLISIS DIMENSIONAL (6 Dimensiones OBLIGATORIAS)

Para CADA unidad identificada, codifica las 6 dimensiones:

### DIMENSI√ìN 1: CORPORAL (Leib / Lived Body)

**Formato**: `[tipo]-[localizaci√≥n]-[intensidad]-[din√°mica]`

**Tipos**: Presi√≥n, Tensi√≥n, Peso, Ligereza, Calor, Fr√≠o, Hormigueo, Pulsaci√≥n, Escalofr√≠os, N√°usea, Dolor, Rigidez, Expansi√≥n, Contracci√≥n

**Localizaci√≥n**: Focal (pecho, nuca, hombros...) o Difusa (generalizada, corporal-total)

**Intensidad**: Muy Baja, Baja, Media, Alta, Muy Alta

**Din√°mica**: Est√°tica, Progresiva, Pulsante, Intermitente, S√∫bita

### DIMENSI√ìN 2: AFECTIVA (Affective Tonality)

**Formato**: `[emoci√≥n]-[calidad]-[intensidad]-[valencia]`

**Emociones**: Curiosidad, Anticipaci√≥n, Asombro, √âxtasis, Alegr√≠a, Calma, Inquietud, Alarma, Ansiedad, Miedo, Terror, P√°nico, Angustia, Confusi√≥n, Alivio, Repulsi√≥n, Malestar

**Valencia**: positiva (+), negativa (-), neutra (0), mixta (¬±)

### DIMENSI√ìN 3: COGNITIVA (Cognitive Activity)

**Formato**: `[tipo]-[contenido]-[tono]`

**Tipos**: Pregunta exploratoria, Catastrofismo, An√°lisis t√©cnico, Memoria epis√≥dica, Suspensi√≥n pensamiento, Met√°fora/Imagen mental, Auto-instrucci√≥n, Narrativa descriptiva

### DIMENSI√ìN 4: MOTIVACIONAL (Action Tendencies)

**Formato**: `impulso-[tipo]-[objeto]-[intensidad]`

**Tipos**: Acercamiento, Evitaci√≥n/Huida, Protecci√≥n, Entrega, B√∫squeda ayuda, Congelamiento/Par√°lisis, Permanencia, Exploraci√≥n activa

### DIMENSI√ìN 5: TEMPORAL (Phase Positioning)

**Formato**: `fase-[nombre-descriptivo]`

**CR√çTICO**: Usa nombres FENOMENOL√ìGICOS (no "Fase 1", "Fase 2")

### DIMENSI√ìN 6: RELACIONAL (Attentional Orientation)

**Formato**: `atencion-[orientaci√≥n]-[objeto]-[cualidad]`

**Orientaciones**: Self-focalizada, Mundo-focalizada, Otro-focalizada, Fluctuante/Mixta, Difusa/No-dual

---

## FORMATO JSON DE SALIDA COMPLETA

```json
{
  "participant_id": "P##",
  "reliability_assessment": {...},
  "chronological_reconstruction": {...},
  "phenomenon_nucleus": "S√≠ntesis narrativa...",
  "dimensional_statistics": {...},
  "markdown_table": "| Unidad | Cita | CORP | AFEC | COG | MOT | TEMP | REL |\\n...",
  "dominant_trajectories": {...}
}
```

================================================================================
FIN PARTE 1 - AN√ÅLISIS INDIVIDUAL
================================================================================
"""

    PARTE_2_EMBEDDED = """
================================================================================
PHENOMFLOW v3.0 - S√çNTESIS CROSS-CASE
PARTE 2: CODEBOOK JER√ÅRQUICO Y CLUSTERING EXPERIENCIAL
================================================================================

## PASO 3.1: CODEBOOK EMERGENTE (4 Niveles Jer√°rquicos)

### Estructura Jer√°rquica Obligatoria

```
NIVEL 1: CATEGOR√çA PRINCIPAL
‚îú‚îÄ NIVEL 2: Subcategor√≠a
‚îÇ  ‚îú‚îÄ NIVEL 3: Especificaci√≥n
‚îÇ  ‚îÇ  ‚îú‚îÄ NIVEL 4: C√≥digo Espec√≠fico + Citas
```

**REGLAS DE VALIDACI√ìN**:
- ‚úì Cada c√≥digo espec√≠fico DEBE tener ‚â•2 citas de ‚â•2 participantes
- ‚úì Cada especificaci√≥n DEBE tener ‚â•2 c√≥digos
- ‚úì Cada subcategor√≠a DEBE tener ‚â•2 especificaciones

## PASO 3.2: ESTRUCTURAS EXPERIENCIALES (Clustering)

**Identificar CLAVE DE PARTICI√ìN**: Categor√≠a descriptiva cuyos valores distribuyen experiencias en clusters.

**Validar coherencia multidimensional**:
- Cada estructura DEBE tener ‚â•75% coherencia en ‚â•4/6 dimensiones

## PASO 3.3: ESTRUCTURA TEMPORAL DIFERENCIADA

Fases atravesadas por ‚â•60% de participantes con manifestaci√≥n diferenciada por estructura.

## FORMATO JSON DE SALIDA

```json
{
  "codebook": {
    "statistics": {...},
    "categories": [...]
  },
  "experiential_structures": [...],
  "differentiated_temporal_structure": [...]
}
```

================================================================================
FIN PARTE 2 - S√çNTESIS CROSS-CASE
================================================================================
"""

    PARTE_3_EMBEDDED = """
================================================================================
PHENOMFLOW v3.0 - VALIDACI√ìN FINAL
PARTE 3: VERIFICACI√ìN, SATURACI√ìN Y CONSISTENCIA
================================================================================

## PASO 4.1: VERIFICACI√ìN DE EVIDENCIA

Para CADA c√≥digo: ‚úì ‚â•2 citas de ‚â•2 participantes diferentes

## PASO 4.2: SATURACI√ìN TEM√ÅTICA

**Criterio**:
- ‚úì COMPLETA: ‚â•90% c√≥digos recurrentes
- ‚ö†Ô∏è PARCIAL: 80-89% c√≥digos recurrentes
- ‚ùå NO SATURACI√ìN: <80%

## PASO 4.3: CONSISTENCIA INTERNA

- Test 1: Mutua Exclusividad de Estructuras
- Test 2: Coherencia de Co-ocurrencias

## CHECKLIST DE AUTO-VERIFICACI√ìN FINAL (45 √≠tems)

**CRITERIO APROBACI√ìN**:
- ‚â•42/45 (93%+): EXCELENTE
- 38-41 (84-91%): BUENO
- 34-37 (76-83%): ACEPTABLE
- <34 (<76%): REQUIERE REVISI√ìN

================================================================================
FIN PARTE 3 - VALIDACI√ìN
================================================================================
"""

    return PARTE_1_EMBEDDED, PARTE_2_EMBEDDED, PARTE_3_EMBEDDED


# Cargar prompts al iniciar
PROMPT_PARTE_1, PROMPT_PARTE_2, PROMPT_PARTE_3 = load_prompt_parts()


# =============================================================================
# FUNCIONES AUXILIARES
# =============================================================================

def call_llm(prompt: str, system_message: str = None, temperature: float = 0.3, 
             max_tokens: int = 16000, json_mode: bool = False) -> str:
    """
    Wrapper unificado para llamadas a Claude o OpenAI.
    """
    max_retries = 5
    base_delay = 10
    
    for attempt in range(max_retries):
        try:
            if USE_CLAUDE:
                # Claude API
                messages = [{"role": "user", "content": prompt}]
                
                response = client.messages.create(
                    model=MODEL,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    system=system_message if system_message else "You are a phenomenological analysis expert following Giorgi & Petitmengin methodology.",
                    messages=messages
                )
                
                text = response.content[0].text
                
                if json_mode:
                    # Clean markdown code blocks if present
                    text = text.strip()
                    if text.startswith("```json"):
                        text = text[7:]
                    elif text.startswith("```"):
                        text = text[3:]
                    if text.endswith("```"):
                        text = text[:-3]
                    text = text.strip()
                    
                return text
            
            else:
                # OpenAI API
                messages = [
                    {"role": "system", "content": system_message if system_message else "You are a phenomenological analysis expert."},
                    {"role": "user", "content": prompt}
                ]
                
                kwargs = {
                    "model": MODEL,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
                
                if json_mode:
                    kwargs["response_format"] = {"type": "json_object"}
                
                response = client.chat.completions.create(**kwargs)
                return response.choices[0].message.content
                
        except Exception as e:
            # Check for rate limit errors
            is_rate_limit = False
            if USE_CLAUDE and isinstance(e, anthropic.RateLimitError):
                is_rate_limit = True
            elif not USE_CLAUDE and "rate_limit" in str(e).lower():
                is_rate_limit = True
                
            if is_rate_limit and attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 5)
                print(f"‚ö†Ô∏è Rate limit hit. Retrying in {delay:.1f}s (Attempt {attempt+1}/{max_retries})...")
                time.sleep(delay)
            else:
                raise e


# =============================================================================
# FUNCIONES DE INTEGRACI√ìN DE CONTEXTO
# =============================================================================

def integrate_research_context(base_prompt: str, context: Dict) -> str:
    """
    Inyecta el contexto de investigaci√≥n en el prompt base.
    
    Args:
        base_prompt: Prompt original de PhenomFlow v3.0
        context: Datos del ResearchContextForm
    
    Returns:
        Prompt enriquecido con contexto
    """
    
    if not context:
        return base_prompt
    
    context_section = "\n\n<RESEARCH_CONTEXT>\n"
    
    if context.get("research_question"):
        context_section += f"**Pregunta de investigaci√≥n:** {context['research_question']}\n\n"
    
    if context.get("study_objective"):
        context_section += f"**Objetivo del estudio:** {context['study_objective']}\n\n"
    
    if context.get("phenomenological_approach"):
        context_section += f"**Enfoque fenomenol√≥gico:** {context['phenomenological_approach']}\n\n"
    
    if context.get("participant_context"):
        context_section += f"**Contexto de participantes:** {context['participant_context']}\n\n"
    
    if context.get("interview_type"):
        context_section += f"**Tipo de entrevista:** {context['interview_type']}\n\n"
    
    if context.get("interview_timing"):
        context_section += f"**Momento de la entrevista:** {context['interview_timing']}\n\n"
    
    context_section += """
**INSTRUCCI√ìN CR√çTICA:** 
Usa este contexto para:
1. Priorizar dimensiones relevantes al fen√≥meno estudiado
2. Ajustar la granularidad del an√°lisis seg√∫n el timing de la entrevista
3. Seleccionar c√≥digos que respondan directamente a la pregunta de investigaci√≥n
4. Adaptar el nivel de interpretaci√≥n seg√∫n el enfoque fenomenol√≥gico
</RESEARCH_CONTEXT>\n\n"""
    
    # Insertar despu√©s del encabezado del prompt
    if "<TASK>" in base_prompt:
        return base_prompt.replace("<TASK>", context_section + "<TASK>")
    else:
        return context_section + base_prompt


# =============================================================================
# FUNCIONES DE GENERACI√ìN DE BODY MAPS
# =============================================================================

def generate_body_maps(codebook: dict, clustering: dict) -> dict:
    """
    Genera body maps organizados por estructura experiencial.
    
    Args:
        codebook: Libro de c√≥digos jer√°rquico
        clustering: Resultados de clustering con estructuras
    
    Returns:
        dict con formato: {"structures": [{...}]}
    """
    
    # Mapeo zona corporal ‚Üí keywords
    ZONE_MAPPING = {
        "head": ["cabeza", "craneal", "cefalico", "frontal", "temporal", "occipital", "ojo", "ocular", "frente", "sien"],
        "neck": ["cuello", "cervical", "garganta", "nuca"],
        "chest": ["pecho", "torax", "toracico", "costal", "esternal", "pulmon", "corazon"],
        "solar_plexus": ["plexo", "epigastrio", "epigastrico", "diafragma", "boca-estomago"],
        "abdomen": ["abdomen", "abdominal", "estomago", "gastrico", "vientre", "intestin", "visceral"],
        "pelvis": ["pelvis", "pelvico", "bajo-vientre", "genital", "cadera-baja"],
        "extremities": ["mano", "brazo", "pierna", "pie", "tobillo", "muneca", 
                       "codo", "rodilla", "hombro", "extremidad", "dedo", "cadera"]
    }
    
    try:
        structures = clustering.get("structures", [])
        if not structures:
            return {"structures": []}
        
        body_map_structures = []
        
        for struct in structures:
            struct_id = struct.get("structure_id")
            struct_name = struct.get("structure_name", f"Estructura {struct_id}")
            participants = struct.get("participants", [])
            
            # Inicializar zonas
            zones = {zone: {"count": 0, "codes": [], "quotes": []} 
                    for zone in ZONE_MAPPING.keys()}
            
            # Buscar c√≥digos CORPORALES en el codebook
            corporal_category = codebook.get("CORPORAL", {})
            
            for subcat_name, subcat_data in corporal_category.items():
                if not isinstance(subcat_data, dict):
                    continue
                    
                for spec_name, spec_data in subcat_data.items():
                    if not isinstance(spec_data, list):
                        continue
                        
                    for code_entry in spec_data:
                        code = code_entry.get("code", "")
                        code_participants = code_entry.get("participants", [])
                        evidence = code_entry.get("evidence", [])
                        
                        # Filtrar por participantes de esta estructura
                        struct_matches = [p for p in code_participants if p in participants]
                        
                        if struct_matches:
                            # Detectar zona anat√≥mica
                            code_lower = code.lower()
                            zone_found = None
                            
                            for zone, keywords in ZONE_MAPPING.items():
                                if any(kw in code_lower for kw in keywords):
                                    zone_found = zone
                                    break
                            
                            if zone_found:
                                # Agregar referencia por cada participante
                                for pid in struct_matches:
                                    freq = code_participants.count(pid)
                                    zones[zone_found]["codes"].append({
                                        "code": code,
                                        "participant_id": pid,
                                        "frequency": freq
                                    })
                                    zones[zone_found]["count"] += freq
                                
                                # Agregar quote (m√°ximo 3 por zona)
                                if evidence and len(zones[zone_found]["quotes"]) < 3:
                                    for ev in evidence[:3]:
                                        if ev not in zones[zone_found]["quotes"]:
                                            zones[zone_found]["quotes"].append(ev)
            
            # Filtrar zonas vac√≠as
            zones = {k: v for k, v in zones.items() if v["count"] > 0}
            
            body_map_structures.append({
                "structure_id": struct_id,
                "structure_name": struct_name,
                "participants": participants,
                "zones": zones
            })
        
        return {"structures": body_map_structures}
    
    except Exception as e:
        print(f"‚ùå Error generando body maps: {e}")
        import traceback
        traceback.print_exc()
        return {"structures": []}


# =============================================================================
# FUNCIONES PRINCIPALES DE AN√ÅLISIS (del c√≥digo original)
# =============================================================================

def analyze_individual_interview(text: str, participant_id: str = "Pxx") -> Dict[str, Any]:
    """
    FASE 1: An√°lisis individual con prompt v3.0 completo.
    """
    
    print(f"\nüîç Analizando {participant_id}...")
    
    # Construir prompt completo
    full_prompt = f"""{PROMPT_PARTE_1}

================================================================================
AN√ÅLISIS DE PARTICIPANTE {participant_id}
================================================================================

TRANSCRIPCI√ìN ORIGINAL:

{text}

================================================================================

INSTRUCCIONES FINALES:

1. Aplica TODA la metodolog√≠a descrita en PARTE 1
2. Genera an√°lisis completo en formato JSON (usa el schema proporcionado)
3. NO omitas ninguna secci√≥n (confiabilidad, reorganizaci√≥n, tabla, estad√≠sticas, n√∫cleo)
4. S√© RIGUROSO con formatos de c√≥digos (exactamente como se especifica)
5. Reporta [No mencionado] cuando dimensi√≥n ausente (NO inventes)

RETORNA SOLO JSON V√ÅLIDO (sin preamble, sin markdown):
"""
    
    # Llamada al LLM
    response_text = call_llm(
        prompt=full_prompt,
        system_message="You are an expert in Giorgi's descriptive phenomenological method. Return ONLY valid JSON.",
        temperature=0.2,
        max_tokens=16000,
        json_mode=True
    )
    
    # Parse JSON
    try:
        result = json.loads(response_text)
        result["participant_id"] = participant_id  # Asegurar ID correcto
        
        print(f"‚úÖ {participant_id} analizado")
        return result
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parseando JSON para {participant_id}: {e}")
        return {
            "participant_id": participant_id,
            "error": str(e),
            "raw_response": response_text[:500]
        }


def perform_cross_case_synthesis(analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    FASE 2: S√≠ntesis cross-case con prompt v3.0 completo.
    """
    
    print(f"\nüîÑ Iniciando s√≠ntesis cross-case de {len(analyses)} participantes...")
    
    # Preparar res√∫menes
    summaries = []
    for analysis in analyses:
        pid = analysis.get('participant_id', 'Unknown')
        nucleus = analysis.get('phenomenon_nucleus', 'N/A')
        stats = analysis.get('dimensional_statistics', {})
        
        summary = f"""
PARTICIPANTE {pid}:
- N√∫cleo fenomenol√≥gico: {nucleus}
- Tabla: {analysis.get('markdown_table', 'N/A')[:500]}...
"""
        summaries.append(summary)
    
    combined_summary = "\n\n".join(summaries)
    
    full_prompt = f"""{PROMPT_PARTE_2}

================================================================================
S√çNTESIS CROSS-CASE DE {len(analyses)} PARTICIPANTES
================================================================================

AN√ÅLISIS INDIVIDUALES:

{combined_summary}

RETORNA SOLO JSON V√ÅLIDO (sin preamble, sin markdown):
"""
    
    response_text = call_llm(
        prompt=full_prompt,
        system_message="You are an expert in phenomenological synthesis. Return ONLY valid JSON with complete codebook.",
        temperature=0.2,
        max_tokens=16000,
        json_mode=True
    )
    
    try:
        result = json.loads(response_text)
        print(f"‚úÖ S√≠ntesis completada")
        return result
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parseando JSON de s√≠ntesis: {e}")
        return {
            "error": str(e),
            "raw_response": response_text[:500]
        }


def perform_validation(synthesis_result: Dict[str, Any], 
                      individual_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    FASE 3: Validaci√≥n completa con prompt v3.0.
    """
    
    print(f"\n‚úì Iniciando validaci√≥n final...")
    
    codebook_summary = json.dumps(synthesis_result.get('codebook', {}), indent=2)[:2000]
    
    full_prompt = f"""{PROMPT_PARTE_3}

================================================================================
VALIDACI√ìN FINAL - {len(individual_analyses)} PARTICIPANTES
================================================================================

CODEBOOK GENERADO (primeras 2000 chars):
{codebook_summary}

RETORNA JSON CON:
- saturation: {{achieved: bool, percentage: number}}
- consistency_tests: {{intercoder: {{passed: bool, score: number}}, intracoder: {{...}}}}
- checklist_score: number (0-45)
"""
    
    response_text = call_llm(
        prompt=full_prompt,
        system_message="You are a validation expert. Return ONLY valid JSON with complete validation results.",
        temperature=0.1,
        max_tokens=8000,
        json_mode=True
    )
    
    try:
        result = json.loads(response_text)
        print(f"‚úÖ Validaci√≥n completada: {result.get('checklist_score', '?')}/45")
        return result
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parseando validaci√≥n: {e}")
        return {"error": str(e)}


# =============================================================================
# FLASK ENDPOINTS
# =============================================================================

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "model": MODEL,
        "use_claude": USE_CLAUDE,
        "timestamp": datetime.now().isoformat()
    }), 200


@app.route('/analyze', methods=['POST'])
def analyze_basic():
    """
    Endpoint b√°sico sin contexto de investigaci√≥n (retrocompatibilidad).
    
    Request: {"text": "U1: ... U2: ..."}
    Response: An√°lisis individual
    """
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({"error": "Missing 'text' field"}), 400
        
        text = data['text']
        pid = data.get('participant_id', 'P01')
        
        result = analyze_individual_interview(text, pid)
        
        return jsonify(result), 200
    
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route('/analyze/enhanced', methods=['POST'])
def analyze_enhanced():
    """
    Endpoint mejorado que acepta contexto de investigaci√≥n.
    
    Request body:
    {
        "text": "U1: ... U2: ...",
        "context": {
            "research_question": "...",
            "study_objective": "...",
            "phenomenological_approach": "...",
            ...
        }
    }
    
    Response:
    {
        "codebook": {...},
        "temporal_structures": {...},
        "clustering": {...},
        "body_maps": {...},
        "validation": {...},
        "metadata": {...}
    }
    """
    
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({"error": "Missing 'text' field"}), 400
        
        text = data['text']
        context = data.get('context', None)
        
        # Validar que hay contenido
        if not text.strip():
            return jsonify({"error": "Empty text provided"}), 400
        
        print(f"\nüîç Starting enhanced analysis...")
        print(f"üìÑ Text length: {len(text)} characters")
        if context:
            print(f"üéØ Research context: {context.get('phenomenological_approach', 'N/A')}")
        
        # INTEGRAR CONTEXTO EN PROMPTS
        prompt_part1 = PROMPT_PARTE_1
        if context:
            prompt_part1 = integrate_research_context(prompt_part1, context)
        
        # FASE 1: An√°lisis Individual (asumimos 1 participante por ahora)
        # Si quieres analizar m√∫ltiples, necesitas parsear el texto
        result = analyze_individual_interview(text, "P01")
        
        # Para an√°lisis completo cross-case necesitamos m√∫ltiples participantes
        # Por ahora, devolvemos estructura simulada
        
        # GENERAR BODY MAPS (simulado para 1 participante)
        body_maps = {
            "structures": [
                {
                    "structure_id": 1,
                    "structure_name": "An√°lisis Individual",
                    "participants": ["P01"],
                    "zones": {}
                }
            ]
        }
        
        # METADATA
        result["metadata"] = {
            "analysis_date": datetime.now().isoformat(),
            "model": MODEL,
            "phenomflow_version": "3.0",
            "context_used": context is not None,
            "text_length": len(text)
        }
        
        # Estructura temporal (del an√°lisis individual)
        result["temporal_structures"] = {
            "P01": {
                "participant_id": "P01",
                "phases": [],  # Extraer de an√°lisis
                "nuclear_metaphors": [],
                "inflection_points": []
            }
        }
        
        # Clustering (simulado para 1 participante)
        result["clustering"] = {
            "structures": [
                {
                    "structure_id": 1,
                    "structure_name": "An√°lisis Individual",
                    "participants": ["P01"],
                    "description": result.get("phenomenon_nucleus", ""),
                    "shared_patterns": []
                }
            ],
            "dimensions": {}  # Extraer de dimensional_statistics
        }
        
        # Codebook (estructura b√°sica)
        result["codebook"] = {
            "CORPORAL": {},
            "AFECTIVA": {},
            "COGNITIVA": {},
            "MOTIVACIONAL": {},
            "TEMPORAL": {},
            "RELACIONAL": {}
        }
        
        # Validaci√≥n (simulada)
        result["validation"] = {
            "saturation": {"achieved": False, "percentage": 0},
            "consistency_tests": {
                "intercoder": {"passed": False, "score": 0},
                "intracoder": {"passed": False, "score": 0}
            },
            "checklist_score": 0
        }
        
        # Body maps
        result["body_maps"] = body_maps
        
        print(f"‚úÖ Analysis complete!")
        return jsonify(result), 200
    
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route('/analyze/document', methods=['POST'])
def analyze_document():
    """
    Endpoint para analizar documentos cargados.
    """
    # TODO: Implementar an√°lisis de documentos PDF/DOCX
    return jsonify({"error": "Not implemented yet"}), 501


# =============================================================================
# FUNCIONES WRAPPER (Compatibilidad con c√≥digo original)
# =============================================================================

def run_complete_pipeline(transcripts: List[Dict[str, str]], 
                         output_dir: str = "./analysis_results") -> str:
    """
    Ejecuta pipeline completo v3.0.
    """
    
    print("\n" + "="*80)
    print("PHENOMFLOW v3.0 - PIPELINE COMPLETO")
    print("="*80)
    
    # FASE 1: An√°lisis Individual
    individual_results = []
    for t in transcripts:
        result = analyze_individual_interview(t['text'], t['participant_id'])
        individual_results.append(result)
    
    # FASE 2: S√≠ntesis Cross-Case
    synthesis_result = perform_cross_case_synthesis(individual_results)
    
    # FASE 3: Validaci√≥n
    validation_result = perform_validation(synthesis_result, individual_results)
    
    # GENERAR BODY MAPS
    if "codebook" in synthesis_result and "experiential_structures" in synthesis_result:
        body_maps = generate_body_maps(
            synthesis_result["codebook"], 
            {"structures": synthesis_result["experiential_structures"]}
        )
        synthesis_result["body_maps"] = body_maps
    
    print(f"\n‚úÖ PIPELINE COMPLETADO")
    return synthesis_result


# =============================================================================
# ENTRY POINT
# =============================================================================

if __name__ == "__main__":
    # Si se ejecuta directamente, iniciar servidor Flask
    port = int(os.getenv("PORT", 8000))
    print(f"\nüöÄ Starting PhenomFlow v3.0 API Server on port {port}...")
    print(f"üì° CORS enabled for: http://localhost:3000")
    print(f"ü§ñ Model: {MODEL}")
    print(f"\nüí° Endpoints disponibles:")
    print(f"   GET  /health")
    print(f"   POST /analyze")
    print(f"   POST /analyze/enhanced")
    print(f"   POST /analyze/document")
    print(f"\n‚è∞ Server starting...\n")
    
    app.run(host='0.0.0.0', port=port, debug=True)